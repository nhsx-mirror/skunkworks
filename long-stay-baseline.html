<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@NHSuk"/><meta name="twitter:creator" content="@NHSuk"/><meta property="og:url" content="https://nhsx.github.io/skunkworks/"/><meta property="og:image" content="https://nhsx.github.io/skunkworks/social-cover.jpg"/><meta property="og:image:alt" content="NHS AI Lab Skunkworks Social Cover"/><meta property="og:image:type" content="image/jpeg"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><title>Long Stayer Risk Stratification Baseline Models | NHS AI Lab Skunkworks</title><meta name="robots" content="index,follow"/><meta name="description" content="Baseline machine learning models using historical data from Gloucestershire Hospitals NHS Foundation Trust to predict how long a patient will stay in hospital upon admission."/><meta property="og:title" content="Long Stayer Risk Stratification Baseline Models | NHS AI Lab Skunkworks"/><meta property="og:description" content="Baseline machine learning models using historical data from Gloucestershire Hospitals NHS Foundation Trust to predict how long a patient will stay in hospital upon admission."/><meta name="next-head-count" content="16"/><link rel="icon" href="/favicon.ico"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/skunkworks/_next/static/css/1e24fbca24494117.css" as="style"/><link rel="stylesheet" href="/skunkworks/_next/static/css/1e24fbca24494117.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/skunkworks/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/skunkworks/_next/static/chunks/webpack-382465b464c37bb5.js" defer=""></script><script src="/skunkworks/_next/static/chunks/framework-5f4595e5518b5600.js" defer=""></script><script src="/skunkworks/_next/static/chunks/main-a05df26ae4493c52.js" defer=""></script><script src="/skunkworks/_next/static/chunks/pages/_app-3fabb4cbd231f59d.js" defer=""></script><script src="/skunkworks/_next/static/chunks/622-6b0ef3ffcb46ab6b.js" defer=""></script><script src="/skunkworks/_next/static/chunks/234-830443fac4b3a699.js" defer=""></script><script src="/skunkworks/_next/static/chunks/pages/%5Bslug%5D-7385ec9ab8515315.js" defer=""></script><script src="/skunkworks/_next/static/g3Eb-oK66BnMwIyIlPHDf/_buildManifest.js" defer=""></script><script src="/skunkworks/_next/static/g3Eb-oK66BnMwIyIlPHDf/_ssgManifest.js" defer=""></script><script src="/skunkworks/_next/static/g3Eb-oK66BnMwIyIlPHDf/_middlewareManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Inter:wght@200;300;400;500;600;700&display=swap">@font-face{font-family:'Inter';font-style:normal;font-weight:200;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuDyfMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuOKfMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuLyfMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuI6fMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuGKYMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuFuYMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:200;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:200;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:200;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:200;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:200;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:200;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:200;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-02AF,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v12/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id="__next" data-reactroot=""><div class="antialiased"><div class="min-h-full"><nav class="bg-blue-500"><div class="mx-auto px-4 sm:px-6 lg:px-8 max-w-6xl"><div class="flex justify-between py-6"><div class="flex"><a class="flex flex-col space-y-6 md:space-y-4 lg:space-y-0 lg:flex-row lg:items-center lg:space-x-2 flex-shrink-0 text-white" href="/skunkworks"><span><img class="h-10 w-auto" src="/skunkworks/nhs-logo-inverted.svg" alt="NHS"/></span><span class="text-sm lg:text-lg">AI Lab Skunkworks</span></a></div><div class="hidden md:flex"><div class="flex-1 flex items-center space-x-6"><form action="/skunkworks/search" method="GET" class="flex-1 justify-stretch relative flex"><div class="flex-1"><input type="text" name="q" placeholder="Search" class="w-full border-2 border-transparent focus:ring-nhsuk-focus focus:ring-4 rounded-tl rounded-bl sm:text-sm" value=""/></div><div><button type="submit" class="w-full h-full flex-1 bg-gray-50 rounded-tr rounded-br px-3 text-blue-500 focus:bg-nhsuk-yellow focus:text-black "><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="w-6 h-6"><path fill-rule="evenodd" d="M8 4a4 4 0 100 8 4 4 0 000-8zM2 8a6 6 0 1110.89 3.476l4.817 4.817a1 1 0 01-1.414 1.414l-4.816-4.816A6 6 0 012 8z" clip-rule="evenodd"></path></svg></button></div></form><a target="_BLANK" class="rounded-full" href="https://github.com/nhsx/skunkworks"><svg class="w-10 h-10 flex-shrink-0 text-white" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 17 16" fill="none"><g clip-path="url(githublogo)"><path fill="currentColor" fill-rule="evenodd" d="M8.18391.249268C3.82241.249268.253906 3.81777.253906 8.17927c0 3.46933 2.279874 6.44313 5.451874 7.53353.3965.0991.49563-.1983.49563-.3965v-1.3878c-2.18075.4956-2.67638-.9912-2.67638-.9912-.3965-.8922-.89212-1.1895-.89212-1.1895-.69388-.4957.09912-.4957.09912-.4957.793.0992 1.1895.793 1.1895.793.69388 1.2887 1.88338.8922 2.27988.6939.09912-.4956.29737-.8921.49562-1.0904-1.78425-.1982-3.5685-.8921-3.5685-3.96496 0-.89212.29738-1.586.793-2.08162-.09912-.19825-.3965-.99125.09913-2.08163 0 0 .69387-.19825 2.18075.793.59475-.19825 1.28862-.29737 1.9825-.29737.69387 0 1.38775.09912 1.98249.29737 1.4869-.99125 2.1808-.793 2.1808-.793.3965 1.09038.1982 1.88338.0991 2.08163.4956.59475.793 1.28862.793 2.08162 0 3.07286-1.8834 3.66766-3.66764 3.86586.29737.3965.59474.8921.59474 1.586v2.1808c0 .1982.0991.4956.5948.3965 3.172-1.0904 5.4518-4.0642 5.4518-7.53353-.0991-4.3615-3.6676-7.930002-8.02909-7.930002z" clip-rule="evenodd" class="jsx-1651122719"></path></g><defs><clipPath id="githublogo"><path fill="transparent" d="M0 0h15.86v15.86H0z" transform="translate(.253906 .0493164)"></path></clipPath></defs></svg></a></div></div><div class="-mr-2 flex items-start md:hidden"><button class="bg-white inline-flex items-center justify-center p-2 text-nhsuk-text hover:bg-gray-100 focus:bg-nhsuk-yellow focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-nhsuk-yellow" id="headlessui-disclosure-button-undefined" type="button" aria-expanded="false"><span class="sr-only">Open main menu</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" aria-hidden="true" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div><div class="hidden md:block"><div class="border-t border-white border-opacity-20"><nav class="flex flex-col space-y-2 md:space-y-0 md:flex-row md:space-x-6"><a class="border-transparent group flex items-center px-4 py-4 border-l-4 md:border-l-0 text-white hover:underline text-base focus:bg-nhsuk-yellow focus:text-black focus:border-black md:focus:border-b-4 md:focus:pb-3" href="/skunkworks">Overview</a><a class="md:border-b-4 md:pb-3 border-white font-semibold group flex items-center px-4 py-4 border-l-4 md:border-l-0 text-white hover:underline text-base focus:bg-nhsuk-yellow focus:text-black focus:border-black md:focus:border-b-4 md:focus:pb-3" href="/skunkworks/data-lens">Projects</a><a class="border-transparent group flex items-center px-4 py-4 border-l-4 md:border-l-0 text-white hover:underline text-base focus:bg-nhsuk-yellow focus:text-black focus:border-black md:focus:border-b-4 md:focus:pb-3" href="/skunkworks/ai-deep-dive">Playbooks</a><a class="border-transparent group flex items-center px-4 py-4 border-l-4 md:border-l-0 text-white hover:underline text-base focus:bg-nhsuk-yellow focus:text-black focus:border-black md:focus:border-b-4 md:focus:pb-3" href="/skunkworks/casestudy-parkinsons-detection">Case Study Archive</a><a class="border-transparent group flex items-center px-4 py-4 border-l-4 md:border-l-0 text-white hover:underline text-base focus:bg-nhsuk-yellow focus:text-black focus:border-black md:focus:border-b-4 md:focus:pb-3" href="/skunkworks/team">Team</a></nav></div></div></div></nav><div class=""><main class="flex-1"><div class="mx-auto px-4 sm:px-6 lg:px-8 max-w-6xl"><div class="flex flex-col py-8 lg:flex-row lg:py-12"><div class="hidden lg:flex relative w-80 flex-shrink-0"><div class="-mt-6"><div class="sticky top-0 pt-6"><div class="flex-grow flex flex-col space-y-8"><div class="space-y-4"><div class="flex items-center space-x-3"><div><div class="bg-blue-500 rounded p-1"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="w-4 h-4 text-white"><path fill-rule="evenodd" d="M7 2a1 1 0 00-.707 1.707L7 4.414v3.758a1 1 0 01-.293.707l-4 4C.817 14.769 2.156 18 4.828 18h10.343c2.673 0 4.012-3.231 2.122-5.121l-4-4A1 1 0 0113 8.172V4.414l.707-.707A1 1 0 0013 2H7zm2 6.172V4h2v4.172a3 3 0 00.879 2.12l1.027 1.028a4 4 0 00-2.171.102l-.47.156a4 4 0 01-2.53 0l-.563-.187a1.993 1.993 0 00-.114-.035l1.063-1.063A3 3 0 009 8.172z" clip-rule="evenodd"></path></svg></div></div><div><p class="font-semibold text-gray-400 uppercase tracking-wider text-xs">Projects</p></div></div><nav class="mb-4 space-y-1 border-l border-gray-200 ml-3"><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/data-lens">Data Lens</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/long-stay">Long Stayer Risk Stratification</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/ct-alignment">CT Alignment and Lesion Detection</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/bed-allocation">Bed allocation</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/ai-dictionary">AI Dictionary</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/synthetic-data-pipeline">Synthetic Data</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/nursing-placement-optimisation">Nursing Placement Schedule Optimisation</a><a class="underline font-medium -ml-1 pl-7 text-blue-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/long-stay-baseline"><div class="flex absolute -left-1 top-1/2 -mt-2 rounded-full border w-4 h-4 bg-white p-px"><div class="rounded-full bg-blue-500 flex-1"></div></div>Long Stayer Risk Stratification baseline</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/ambulance-delay-predictor">Ambulance Handover Delay Predictor</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/parkinsons-detection">Parkinson&#x27;s Disease Pathology Prediction</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/renal-health-prediction">Renal Health Prediction</a></nav></div><div class="space-y-4"><div class="flex items-center space-x-3"><div><div class="bg-blue-500 rounded p-1"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="w-4 h-4 text-white"><path d="M9 4.804A7.968 7.968 0 005.5 4c-1.255 0-2.443.29-3.5.804v10A7.969 7.969 0 015.5 14c1.669 0 3.218.51 4.5 1.385A7.962 7.962 0 0114.5 14c1.255 0 2.443.29 3.5.804v-10A7.968 7.968 0 0014.5 4c-1.255 0-2.443.29-3.5.804V12a1 1 0 11-2 0V4.804z"></path></svg></div></div><div><p class="font-semibold text-gray-400 uppercase tracking-wider text-xs">Playbooks</p></div></div><nav class="mb-4 space-y-1 border-l border-gray-200 ml-3"><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/ai-deep-dive">AI Deep Dive</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/project-flow">AI Prototype Project Flow</a></nav></div><div class="space-y-4"><div class="flex items-center space-x-3"><div><div class="bg-blue-500 rounded p-1"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="w-4 h-4 text-white"><path d="M9 4.804A7.968 7.968 0 005.5 4c-1.255 0-2.443.29-3.5.804v10A7.969 7.969 0 015.5 14c1.669 0 3.218.51 4.5 1.385A7.962 7.962 0 0114.5 14c1.255 0 2.443.29 3.5.804v-10A7.968 7.968 0 0014.5 4c-1.255 0-2.443.29-3.5.804V12a1 1 0 11-2 0V4.804z"></path></svg></div></div><div><p class="font-semibold text-gray-400 uppercase tracking-wider text-xs">Case Study Archive</p></div></div><nav class="mb-4 space-y-1 border-l border-gray-200 ml-3"><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/casestudy-parkinsons-detection">Parkinson&#x27;s Disease Pathology Prediction</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/casestudy-nursing-placement-optimisation">Nursing Placement Schedule Optimisation</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/casestudy-synthetic-data-pipeline">Synthetic Data</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/casestudy-bed-allocation">Bed allocation</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/casestudy-ct-alignment">CT Alignment and Lesion Detection</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/casestudy-data-lens">Data Lens</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/casestudy-nhs-resolution">Negligence Claims Prediction</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/casestudy-long-stay">Long Stayer Risk Stratification</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/casestudy-opensafely">Open Safely</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/casestudy-adrenal">Adrenal Incidentalomas Detection</a><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/casestudy-ai-deep-dive">AI Deep Dive at UHS</a></nav></div><div class="space-y-4"><div class="flex items-center space-x-3"><div><div class="bg-blue-500 rounded p-1"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="w-4 h-4 text-white"><path d="M9 6a3 3 0 11-6 0 3 3 0 016 0zM17 6a3 3 0 11-6 0 3 3 0 016 0zM12.93 17c.046-.327.07-.66.07-1a6.97 6.97 0 00-1.5-4.33A5 5 0 0119 16v1h-6.07zM6 11a5 5 0 015 5v1H1v-1a5 5 0 015-5z"></path></svg></div></div><div><p class="font-semibold text-gray-400 uppercase tracking-wider text-xs">Team</p></div></div><nav class="mb-4 space-y-1 border-l border-gray-200 ml-3"><a class="text-gray-500 relative group flex items-center py-2 px-6 text-base" href="/skunkworks/team">Team</a></nav></div></div></div></div></div><div class="lg:hidden flex flex-col mb-8"><button class="bg-gray-100 flex justify-center items-center p-4 text-gray-500 underline" id="headlessui-disclosure-button-undefined" type="button" aria-expanded="false">Show menu</button></div><div class="flex-1 space-y-10"><div class="prose max-w-none"><h1>Long Stayer Risk Stratification Baseline Models</h1><blockquote><p>Baseline machine learning models using historical data from Gloucestershire Hospitals NHS Foundation Trust to predict how long a patient will stay in hospital upon admission.</p></blockquote><div class=""><div class="flex gap-2"><div class="px-3 py-1 bg-gray-100 text-gray-500 text-sm font-medium rounded">LoS</div><div class="px-3 py-1 bg-gray-100 text-gray-500 text-sm font-medium rounded">length of stay</div><div class="px-3 py-1 bg-gray-100 text-gray-500 text-sm font-medium rounded">baseline</div><div class="px-3 py-1 bg-gray-100 text-gray-500 text-sm font-medium rounded">risk model</div><div class="px-3 py-1 bg-gray-100 text-gray-500 text-sm font-medium rounded">regression</div><div class="px-3 py-1 bg-gray-100 text-gray-500 text-sm font-medium rounded">classification</div></div></div>
<p>Long Stayer risk stratification baseline models was selected as a project to run in tandem with the <a href="long-stay">Long Stayer Risk Stratification</a> project, and started in March 2022.</p>
<p>Baseline models provide a mechanism to generate baseline metrics to assess the performance of more complex models, and establish the effectiveness of simple approaches.</p>
<div class="relative mt-10"><div class="inline-block bg-nhsuk-yellow text-nhsuk-text p-8 py-3 text-xl font-semibold">Intended audience</div><div class="p-8 text-lg text-nhsuk-text bg-nhsuk-pale-yellow border border-nhsuk-yellow pt-10 -mt-6">This report has been written for analysts and data scientists at NHS Trusts/ALBs</div></div>
<p>A series of Jupyter Notebooks used to generate this report are available on <a href="https://github.com/nhsx/skunkworks-long-stayer-risk-stratification-baseline/tree/main/notebooks">Github</a>.</p>
<h2>Table of contents</h2>
<ul>
<li>
<ol>
<li><a href="#background">Background</a></li>
</ol>
</li>
<li>
<ol start="2">
<li><a href="#approach">Approach</a></li>
</ol>
</li>
<li>
<ol start="3">
<li><a href="#dataingestandprocessing">Data ingest and processing</a></li>
</ol>
</li>
<li>
<ol start="4">
<li><a href="#featureengineering">Feature engineering</a></li>
</ol>
</li>
<li>
<ol start="5">
<li><a href="#statisticalanalysis">Statistical analysis</a></li>
</ol>
</li>
<li>
<ol start="6">
<li><a href="#modelling">Modelling</a></li>
</ol>
<ul>
<li>6.1 <a href="#regressionmodels">Regression models</a></li>
<li>6.2 <a href="#demographicanalysis">Demographic analysis</a></li>
<li>6.3 <a href="#classificationmodels">Classification models</a></li>
<li>6.4 <a href="#modelcomparison">Model comparison</a></li>
</ul>
</li>
<li>
<ol start="7">
<li><a href="#conclusions">Conclusions</a></li>
</ol>
</li>
<li>
<ol start="8">
<li><a href="#futurework">Future work</a></li>
</ol>
</li>
</ul>
<h2>1. Background<a name="background"></a></h2>
<p>Hospital long stayers, those with a <a href="https://www.england.nhs.uk/urgent-emergency-care/reducing-length-of-stay/" target="_blank">length of stay (LoS) of 21 days or longer</a>, have significantly worse medical and social outcomes than other patients. Long-stayers are often medically optimised (fit for discharge) many days before their actual discharge. Moreover, there are a complex mixture of medical, cultural and socioeconomic factors which contribute to the causes of unnecessary long stays.</p>
<p>This project aims to complement <a href="long-stay">previous work</a> by generating simple baseline regression and classification models that could be replicated at other hospital trusts, and is divided into two phases:</p>
<ol>
<li>Series of Jupyter Notebooks containing baseline model code</li>
<li><a href="https://github.com/NHSDigital/rap-community-of-practice">Reproducible Analytical Pipeline</a> including data pipelines</li>
</ol>
<p>Currently, this project has completed <strong>Phase 1</strong>.</p>
<h2>2. Approach<a name="approach"></a></h2>
<p>The aim of this project is to perform the simplest possible feature engineering and modelling to arrive at a reasonable baseline model, for more advanced feature engineering and modelling to be compared against.</p>
<p>The approach involved:</p>
<ol>
<li>Defining the population and data cleaning</li>
<li>Feature engineering, focussing on basic numerical and categorical features</li>
<li>Simple baseline models implemented using commonly available packages including <a href="https://scikit-learn.org/">scikit-learn 1.1.1</a>, <a href="https://catboost.ai">CatBoost 1.0.6</a> and <a href="https://xgboost.readthedocs.io/en/stable/">XGBoost 1.3.3</a></li>
<li>Analysis of model performance by demographic</li>
<li>A comparison of regression-based and classification-based risk stratification models</li>
<li>A set of extensions for future work</li>
</ol>
<h2>3. Data ingest and processing<a name="dataingestandprocessing"></a></h2>
<p>GHNHSFT performed a SQL export from their EPR system containing ~770,000 records across 99 columns, with significant sparsity across several columns and a section of rows, as visualised by light coloured blocks (null values) in the below image:</p>
<p><img src="images/long-stay-baseline/sparsity.png" alt="Image of data sparsity"/></p>
<p>Figure 1. Plot of data sparsity of raw data. Null values are light coloured blocks. (<em>Note that not all columns are labelled</em>)</p>
<p>The population for this study was defined as non-elective, major cases as recorded in the <code>IS_MAJOR</code> and <code>elective_or_non_elective</code> fields.</p>
<p>Filtering the dataset to this definition resulted in a reduction of ~770,000 rows to ~170,000 rows (78% reduction):</p>
<p><img src="images/long-stay-baseline/sparsity-major.png" alt="Image of data sparsity - major cases"/></p>
<p>Figure 2. Plot of data sparsity of &quot;major&quot; cases. Null values are light coloured blocks. (<em>Note that not all columns are labelled</em>)</p>
<p>Data was processed by:</p>
<ol>
<li>Converting datetime columns into the correct data type</li>
<li>Ordering records by <code>START_DATE_TIME_HOSPITAL_PROVIDER_SPELL</code></li>
<li>Removing fields not available at admission</li>
<li>Removing empty and redundant (e.g. <code>LENGTH_OF_STAY_IN_MINUTES</code> duplicates <code>LENGTH_OF_STAY</code>) columns</li>
<li>Removing duplicate rows</li>
<li>Removing local identifiers</li>
<li>Imputing <code>stroke_ward_stay</code> as <code>N</code> if not specified</li>
<li>Binary encoding <code>stroke_ward_stay</code>, <code>IS_care_home_on_admission</code>, <code>IS_care_home_on_discharge</code> and <code>IS_illness_not_injury</code></li>
</ol>
<p>This resulted in ~170,000 rows across ~50 columns as visualised in the below image:</p>
<p><img src="images/long-stay-baseline/sparsity-clean.png" alt="Image of data sparsity of &quot;clean&quot; data"/></p>
<p>Figure 3. Plot of data sparsity of clean data. Null values are light coloured blocks. (<em>Note that not all columns are labelled</em>)</p>
<p>The resulting data dictionary is available <a href="https://github.com/nhsx/skunkworks-long-stayer-risk-stratification-baseline/blob/main/docs/data-dictionary.csv">here</a>.</p>
<p>Additionally, Length of Stay was capped to 30 days, due to a long tail of long stayers over ~15 days and the definition of long stayer being over 21 days. The effect of capping can be visualised by comparing box plots of the distribution of length of stay on the raw data (left image, y scale up to 300 days) and the capped data (right image, y scale up to 30 days):</p>
<p align="center"><img src="images/long-stay-baseline/los-boxplot.png" alt="Boxplot of length of stay"/></p>
<p>Figure 4. Plot of the distribution of long stayers in the raw (left) data and capped (right) data. Note different y scales.</p>
<p>The resulting distribution of length of stays shows a ~bimodal distribution caused by the capping - the majority of stays are short (&lt;5 days), which a long tail and population of long stayers:</p>
<p align="center"><img src="images/long-stay-baseline/los-density.png" alt="Density plot of length of stay"/></p>
<p>Figure 5. Plot of density of length of stay for capped data.</p>
<h2>4. Feature engineering<a name="featureengineering"></a></h2>
<p>After discussion with GHNHSFT, the following decisions were made in feature selection:</p>
<ol>
<li>Select the following features for inclusion in the model, which are available on admission:</li>
</ol>
<pre><code class="language-python">    &quot;ae_arrival_mode&quot;,
    &quot;AGE_ON_ADMISSION&quot;,
    &quot;EL CountLast12m&quot;,
    &quot;EMCountLast12m&quot;,
    &quot;IS_illness_not_injury&quot;,
    &quot;IS_cancer&quot;,
    &quot;IS_care_home_on_admission&quot;,
    &quot;IS_chronic_kidney_disease&quot;,
    &quot;IS_COPD&quot;,
    &quot;IS_coronary_heart_disease&quot;,
    &quot;IS_dementia&quot;,
    &quot;IS_diabetes&quot;,
    &quot;IS_frailty_proxy&quot;,
    &quot;IS_hypertension&quot;,
    &quot;IS_mental_health&quot;,
    &quot;MAIN_SPECIALTY_CODE_AT_ADMISSION_DESCRIPTION&quot;,
    &quot;OP First CountLast12m&quot;,
    &quot;OP FU CountLast12m&quot;,
    &quot;SOURCE_OF_ADMISSION_HOSPITAL_PROVIDER_SPELL_DESCRIPTION&quot;,
    &quot;stroke_ward_stay&quot;,
    &quot;LENGTH_OF_STAY&quot;,
    &quot;arrival_day_of_week&quot;,
    &quot;arrival_month_name&quot;
</code></pre>
<ol start="2">
<li>Exclude the following features, but retain for later analysis of model fairness:</li>
</ol>
<pre><code class="language-python">    &quot;ETHNIC_CATEGORY_CODE_DESCRIPTION&quot;,
    &quot;IMD county decile&quot;,
    &quot;OAC Group Name&quot;,
    &quot;OAC Subgroup Name&quot;,
    &quot;OAC Supergroup Name&quot;,
    &quot;PATIENT_GENDER_CURRENT_DESCRIPTION&quot;,
    &quot;POST_CODE_AT_ADMISSION_DATE_DISTRICT&quot;,
    &quot;Rural urban classification&quot;
</code></pre>
<ol start="3">
<li>Generate <code>arrival_day_of_week</code> and <code>arrival_month_name</code> recalculated from <code>START_DATE_TIME_HOSPITAL_PROVIDER_SPELL</code></li>
</ol>
<p>This resulted in a dataset of <strong>~170,000 rows across 30 columns</strong>.</p>
<p>One-hot encoding was performed for categorical variables, but non-one-hot encoded features were also kept for models like CatBoost which <a href="https://catboost.ai/en/docs/features/categorical-features">manage categorical variables themselves</a>.</p>
<h3>5. Statistical analysis<a name="statisticalanalysis"></a></h3>
<p>In order to select appropriate modelling approaches, some basic statistical analysis was conducted to understand normality and inter-correlation of the selected features.</p>
<h4>Correlation analysis</h4>
<p>Correlation analysis confirmed presence of significant <strong>collinearity</strong> between different features.</p>
<p>Top 20 one-hot encoded features correlated with <code>LENGTH_OF_STAY</code>, ranked by absolute correlation, were:</p>
<p><img src="images/long-stay-baseline/correlation.png" alt="Plot of correlations with LENGTH_OF_STAY"/></p>
<p>Figure 6. Plot of top 20 correlated features with <code>LENGTH_OF_STAY</code>. Blue columns are positively correlated (ie. increase length of stay) and red columns are negatively correlated (ie. reduce length of stay).</p>
<p>These indicate that age and age-related illness, as well as arrival mode are strong factors in determining length of stay.</p>
<h4>Variation inflation factors</h4>
<p>Variation inflation factors (VIF) confirmed the presence of multi-colinearity between a number of features (VIF &gt; 10).</p>
<h4>Homoescadisticity</h4>
<p>A basic ordinary least squares (OLS) regression model was fitted to the full feature set, then residuals calculated.</p>
<p>Residuals failed Shapiro-Wilk, Kolmogorov-Smirnov and Anderson-Darling tests for normality, as well as visual inspection:</p>
<p><img src="images/long-stay-baseline/residuals.jpeg" alt="Plot of residuals for OLS model of length of stay"/></p>
<p>Figure 7. Plot of residuals (errors) in an OLS model of length of stay for all data.</p>
<p>OLS methods were therefore excluded from modelling.</p>
<h2>6. Modelling<a name="modelling"></a></h2>
<p>The machine learning modelling approach was as follows:</p>
<ol>
<li>Split the data into a training (70%), validation (15%) and test (15%) data set</li>
<li>Check the data splits do not introduce selection bias for length of stay, age, sex, or ethnicity</li>
<li>Train baseline models with default parameters on the training set</li>
<li>Evaluate baseline models on the validation test</li>
<li>Select the best performing model</li>
<li>Tune the best performing model using cross-validation on the training and validation set</li>
<li>Report the final performance of the model using the test set</li>
</ol>
<p><img src="images/long-stay-baseline/ml-approach.png" alt="Summary of machine learning approach"/></p>
<p>Figure 8. Summary of machine learning approach used in this project.</p>
<p>Training, validation and test splits were representative of the population and did not introduce selection bias:</p>
<p><strong>Length of stay</strong></p>
<p><img src="images/long-stay-baseline/split-los.png" alt="Distribution of length of stay by data split"/></p>
<p>Figure 9. Distribution of length of stay by data split.</p>
<p><strong>Age</strong></p>
<p><img src="images/long-stay-baseline/split-age.png" alt="Distribution of age by data split"/></p>
<p>Figure 10. Distribution of age by data split.</p>
<p><strong>Sex</strong></p>
<p>Proportion of <code>male</code>, <code>female</code> patients in each split:</p>
<pre><code>train: [0.53, 0.47]
validate: [0.51, 0.49]
test: [0.53, 0.47]
</code></pre>
<p><strong>Ethnicity</strong></p>
<p>Proportions of each ethnicity for each split:</p>
<pre><code>train: [0.87, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.0, ...]
validate: [0.88, 0.05, 0.03, 0.02, 0.01, 0.01, 0.01, 0.0, ...]
test: [0.87, 0.05, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, ...]
</code></pre>
<h3>6.1 Regression models<a name="regressionmodels"></a></h3>
<p>A range of baseline regression models were selected:</p>
<table><thead><tr><th>Model</th><th>Rationale</th></tr></thead><tbody><tr><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html">Mean</a></td><td>The simplest baseline, uses the mean length of stay as the prediction in all cases</td></tr><tr><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html">ElasticNet</a></td><td>A regularised implementation of linear regression that can be used for multi-colinear datasets such as in this dataset</td></tr><tr><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html">DecisionTreeRegressor</a></td><td>A simple, single tree regressor that is highly explainable</td></tr><tr><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html">RandomForestRegressor</a></td><td>An ensemble of decision trees with potentially better performance than a single tree</td></tr><tr><td><a href="https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor">XGBRegressor</a></td><td>A boosted tree technique that can improve on ensemble techniques such as RandomForest</td></tr><tr><td><a href="https://catboost.ai/en/docs/concepts/python-reference_catboostregressor">CatBoostRegressor</a></td><td>A boosted tree technique designed specifically for datasets with high levels of categorical features as in this dataset</td></tr></tbody></table>
<p>Each model was trained with default parameters, and evaluated using root mean squared error (RMSE) on both the training set and then on the (unseen) validation set:</p>
<table><thead><tr><th>Model</th><th>Training RMSE (days)</th><th>Validation RMSE (days)</th></tr></thead><tbody><tr><td>Mean</td><td>6.89</td><td>6.94</td></tr><tr><td>ElasticNet</td><td>6.55</td><td>6.60</td></tr><tr><td>DecisionTree</td><td>0.55</td><td>9.11</td></tr><tr><td>RandomForest</td><td>2.46</td><td>6.52</td></tr><tr><td>XGBoost</td><td>5.97</td><td>6.32</td></tr><tr><td>CatBoost</td><td>6.13</td><td>6.26</td></tr></tbody></table>
<p>The best performing baseline model was Catboost with an RMSE of 6.26 days. Both DecisionTree and RandomForest models overfit the training data, as seen with low training RMSE resulting in much higher validation RMSE.</p>
<p>A single metric (e.g. RMSE) does not capture the behaviour of each model, so we visualise both the Predicted vs Actual plots as well as the corresponding relative error for both the training set and the validation set:</p>
<p><strong>Training performance:</strong></p>
<p><img src="images/long-stay-baseline/regression-predicted-actuals-training.png" alt="Plots of predicted vs actual and corresponding errors on the training dataset"/></p>
<p>Figure 11. Plots of predicted vs actual (left, red dashed line shows ideal model) and corresponding relative errors (right, red solid line shows mean error with 95% limits of agreement in green dashed lines) on the training dataset.</p>
<p>The RandomForest model appears to fit the training data well, but when compared with the performance on the validation set below, we can see this is due to overfitting on the training data set:</p>
<p><strong>Validation performance:</strong></p>
<p><img src="images/long-stay-baseline/regression-predicted-actuals-validation.jpeg" alt="Plots of predicted vs actual and corresponding errors on the validation dataset"/></p>
<p>Figure 12. Plots of predicted vs actual (left, red dashed line shows ideal model) and corresponding relative errors (right, red solid line shows mean error with 95% limits of agreement in green dashed lines) on the validation dataset.</p>
<p>In all cases, the poor predictive power at higher length of stays is evident - there appears to be a linear increase in error caused by the models&#x27; inability to predict higher length of stays.</p>
<p>This is likely due to the bimodal nature of the underlying length of stay values - most stayers are short, while there is a significant portion of long stayers.</p>
<p>Further tuning of the CatBoost model using GridSearch and cross-validation led to the following results:</p>
<table><thead><tr><th>Parameter</th><th>Optimal value</th></tr></thead><tbody><tr><td><code>depth</code></td><td>6</td></tr><tr><td><code>l2_leaf_reg</code></td><td>9</td></tr><tr><td><code>learning_rate</code></td><td>0.1</td></tr></tbody></table>
<p>with</p>
<table><thead><tr><th>Model</th><th>Training RMSE (days)</th><th>Validation RMSE (days)</th><th>Test RMSE (days)</th><th>Test MAE (days)</th></tr></thead><tbody><tr><td>CatBoost (tuned)</td><td>6.24</td><td>6.18</td><td>6.06</td><td>4.12</td></tr></tbody></table>
<p>The test MAE of 4.12 days compares reasonably well to the previous work using a convolutional neural network which achieved a MAE of 3.8 days.</p>
<p>However, a plot of predicted vs actual using the test dataset shows again the model&#x27;s inability to capture long stayers:</p>
<p><img src="images/long-stay-baseline/regression-predicted-actuals-final-model-test.jpeg" alt="Plots of predicted vs actual and corresponding errors for the final model - test set"/></p>
<p>Figure 13. Plots of predicted vs actual (left, red dashed line shows ideal model) and corresponding relative errors (right, red solid line shows mean error with 95% limits of agreement in green dashed lines) on the test dataset for the final model.</p>
<p>We can still explore the most important features that make up the prediction by plotting feature importances of the final model:</p>
<p><img src="images/long-stay-baseline/regression-feature-importance.png" alt="Feature importances the final model"/></p>
<p>Figure 14. Feature importances for the final regression model.</p>
<p>These broadly align with the correlated features explored earlier on - namely, age, arrival mode, serious illness but also include the number of previous visits, which can be considered a proxy for serious illness itself.</p>
<p>Because the final model, using CatBoost, does not include one-hot encoding of the categorical data as CatBoost deals with this internally, we don&#x27;t have further granularity on admission mode and arrival mode to compare.</p>
<h3>6.2 Demographic analysis<a name="demographicanalysis"></a></h3>
<p>While the model is not peformant enough to deploy into production, it is still important to understand whether or not the model incorporates bias into its predictions.</p>
<p>There are many kinds of bias in machine learning projects, and here we are looking at representation bias:</p>
<blockquote>
<p>Does the model perform better or worse for specific categories of people across sex, ethnicity and other demographics?</p>
</blockquote>
<p>The specific categories are:</p>
<pre><code>&quot;ETHNIC_CATEGORY_CODE_DESCRIPTION&quot;, &quot;IMD county decile&quot;, &quot;OAC Group Name&quot;, &quot;OAC Subgroup Name&quot;, &quot;OAC Supergroup Name&quot;, &quot;PATIENT_GENDER_CURRENT_DESCRIPTION&quot;, &quot;POST_CODE_AT_ADMISSION_DATE_DISTRICT&quot;, &quot;Rural urban classification&quot;
</code></pre>
<p>Before looking at model performance, we need to understand how represented each category is, before drawing conclusions on categories with small sample size (note that for brevity we will only share results from <code>&quot;ETHNIC_CATEGORY_CODE_DESCRIPTION&quot;, &quot;IMD county decile&quot;,&quot;PATIENT_GENDER_CURRENT_DESCRIPTION&quot;</code>):</p>
<p><img src="images/long-stay-baseline/los-dist-ethnicity.png" alt="Underlying counts for ethnicity - all data"/></p>
<p>Figure 15. Underlying counts for ethnicity - all data.</p>
<p>We can see that for <code>ETHNIC_CATEGORY_CODE_DESCRIPTION</code>, the overwhelming majority of patients report <code>British</code>. We should be careful what conclusions we draw in further analysis about smaller categories, as the sample size will be very small and likely not statistically representative.</p>
<p><img src="images/long-stay-baseline/los-dist-sex.png" alt="Underlying counts for sex - all data"/></p>
<p>Figure 16. Underlying counts for sex - all data.</p>
<p>Sex is broadly equal, with slightly more female than male patients in this dataset.</p>
<p><img src="images/long-stay-baseline/los-dist-imd.png" alt="Underlying counts for index of multiple deprivation - all data"/></p>
<p>Figure 17. Underlying counts for index of multiple deprivation - all data</p>
<p>Index of Multiple Deprivation (IMD) deciles are skewed to the lower end, ie. there are more deprived patients present in this dataset than not.</p>
<p>Now we can look at the distribution of length of stay for the above categories:</p>
<p><img src="images/long-stay-baseline/los-mean-los-ethnicity.png" alt="Underlying length of stay by ethnicity - all data"/></p>
<p>Figure 18. Underlying length of stay by ethnicity - all data.</p>
<p>There is significant variation of length of stay for different ethnic groups, for example with White and black Carribean patients having a length of stay of 2.6 days on average, versus 6.0 days for Irish patients. However, as discussed previously, the count of these groups is 560 and 892 individuals respectively so further statistical hypothesis tests need to be conducted to understand if the distributions are truly different (e.g. a two-sided Kolmogorov-Smirnov test).</p>
<p><img src="images/long-stay-baseline/los-mean-los-sex.png" alt="Underlying length of stay by sex - all data"/></p>
<p>Figure 19. Underlying length of stay by sex - all data.</p>
<p>Mean length of stay is almost identical across patient sex.</p>
<p><img src="images/long-stay-baseline/los-mean-los-imd.png" alt="Underlying length of stay by index of multiple deprivation - all data"/></p>
<p>Figure 20. Underlying length of stay by index of multiple deprivation - all data.</p>
<p>There are small variations in length of stay across IMD deciles, although more tests need to be conducted to understand if these differences are statistically significant.</p>
<p>Because we are interested in if the model performs differently by category, we will plot the error of the predictions of the test dataset relative to the overall (mean) error for all categories. This will help identify potential discrimination in model performance.</p>
<p><img src="images/long-stay-baseline/los-rel-error-ethnicity.png" alt="Relative error in length of stay predictions for different ethnic groups - test data"/></p>
<p>Figure 21. Relative error in length of stay predictions for different ethnic groups - test data.</p>
<p>The model appears to perform significantly worse for Carribean (overestimating length of stay by 2.7 days compared to the mean error) and Any other mixed background (underestimating length of stay by 1.8 days compared to the mean error). Sample sizes are 719 and 536 patients respectively. As discussed the small sample sizes need further investigation and/or additional data collection to establish the statistical significance of this performance difference.</p>
<p><img src="images/long-stay-baseline/los-rel-error-sex.png" alt="Relative error in length of stay predictions for different sex - test data"/></p>
<p>Figure 22. Relative error in length of stay predictions for different sex - test data.</p>
<p>Sex has almost no (0.002 days) error from the average.</p>
<p><img src="images/long-stay-baseline/los-rel-error-imd.png" alt="Relative error in length of stay predictions for different index of multiple deprivations deciles - test data"/></p>
<p>Figure 23. Relative error in length of stay predictions for different index of multiple deprivations deciles - test data.</p>
<p>The lowest IMD county decile (1) has an error of 0.5 days underestimating from the mean error, which at under a day may not lead to any difference in treatment if this prediction is used in clinical practice (ie. a length of stay of 1.5 days is the same as a length of stay of 2.0 days - both would count as 2 whole days).</p>
<p>We also know that length of stay varies by group, so further plots of the ratio of MAE to length of stay are generated in the notebooks, but not included here for brevity.</p>
<p>The final model generated did not adequately capture length of stay across the population. Some sample sizes of demographic groups were too small to draw conclusions, but the process of exploring the underlying distribution of the target feature (length of stay), count (n) and model performance were important and should remain part of future work.</p>
<h3>6.3 Classification models<a name="classificationmodels"></a></h3>
<p>In addition to predicting the length of stay in days, we are also interested in stratifying the risk of a patient becoming a long stayer. This can be inferred from their predicted length of stay (see <a href="#modelcomparison">model comparison</a>), but we can also train a classification model to do this directly.</p>
<p>The agreed stratification of risk of long stay is defined as:</p>
<table><thead><tr><th>Risk Category</th><th>Day Range for Risk Category</th></tr></thead><tbody><tr><td>1 - Very low risk</td><td>0-6</td></tr><tr><td>2 - Low risk</td><td>7-10</td></tr><tr><td>3 - Normal risk</td><td>11-13</td></tr><tr><td>4 - Elevated risk</td><td>14-15</td></tr><tr><td>5 - High risk</td><td>&gt;15</td></tr></tbody></table>
<p>We keep the training features the same as in the regression models, and encode risk from the actual length of stay as the target feature.</p>
<blockquote>
<p>Postcript: classification models based on increasing risk (1-5) are ordinal in nature, and an appropriate model should be used where different classes are not treated as independent as per the examples in this implementation.</p>
</blockquote>
<p>The classification equivalents of the regression models were selected:</p>
<table><thead><tr><th>Model</th><th>Regression version</th><th>Classification version</th></tr></thead><tbody><tr><td>Dummy</td><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html">Mean</a></td><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html">Prior</a></td></tr><tr><td>ElasticNet</td><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html">ElasticNet</a></td><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">LogisticRegression</a></td></tr><tr><td>Decision Tree</td><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html">DecisionTreeRegressor</a></td><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">DecisionTreeClassifier</a></td></tr><tr><td>Random Forest</td><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html">RandomForestRegressor</a></td><td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">RandomForestClassifier</a></td></tr><tr><td>XGBoost</td><td><a href="https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor">XGBRegressor</a></td><td><a href="https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier">XGBClassifier</a></td></tr><tr><td>CatBoost</td><td><a href="https://catboost.ai/en/docs/concepts/python-reference_catboostregressor">CatBoostRegressor</a></td><td><a href="https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier">CatBoostClassifier</a></td></tr></tbody></table>
<p>The training, validation and test regime was the same as for the regression models.</p>
<p><strong>Class imbalance</strong></p>
<p>In the training set, we observe the following class imbalance:</p>
<table><thead><tr><th>Risk score</th><th>Number of patients</th><th>% of total patients</th></tr></thead><tbody><tr><td>1</td><td>89711</td><td>74.0</td></tr><tr><td>2</td><td>12634</td><td>10.4</td></tr><tr><td>3</td><td>5226</td><td>9.1</td></tr><tr><td>4</td><td>2613</td><td>4.3</td></tr><tr><td>5</td><td>10990</td><td>2.2</td></tr></tbody></table>
<p>ie. the majority of patients are low risk, and the highest risk group is only 2.2% of the population.</p>
<p>Class and/or sample weights were calculated using the above training imbalances and passed into all models.</p>
<p>Models were trained using default parameters, and evaluated using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">weighted F1 score</a> which represents the balance between precision and recall, and accounts for class imbalance. F1 scores range from 0 to 1 (where 1 is &quot;ideal&quot; or maximum).</p>
<table><thead><tr><th>Model</th><th>Training weighted F1 score</th><th>Validation weighted F1 score</th></tr></thead><tbody><tr><td>Prior</td><td>0.63</td><td>0.62</td></tr><tr><td>ElasticNet</td><td>0.54</td><td>0.53</td></tr><tr><td>DecisionTree</td><td>1.00</td><td>0.59</td></tr><tr><td>RandomForest</td><td>1.00</td><td>0.64</td></tr><tr><td>XGBoost</td><td>0.57</td><td>0.54</td></tr><tr><td>CatBoost</td><td>0.57</td><td>0.54</td></tr></tbody></table>
<p>While the RandomForest model obtained the highest validation weighted F1 score (0.64), it also overfit the training data (weighted F1 score of 1.00).</p>
<p>A visual inspection of model performance, plotting both total counts of risk categories in actual vs predicted cases, as well as the proportion of actual risk in each predicted category, are shown below for both the training and validation data sets:</p>
<p><strong>Training performance:</strong></p>
<p><img src="images/long-stay-baseline/clf-predicted-actuals-training.png" alt="Plots of predicted vs actual risks on the training dataset"/></p>
<p>Figure 24. Plots of predicted vs actual risks on the training dataset. Left image shows count of actual and predicted risks for each category. Right image shows proportion of actual risk that makes up each predicted risk category.</p>
<p>We can see that both the DecisionTree and RandomForest models severely overfit the training data.</p>
<p>We also see that none of the models are able to capture the nature of the highest risk categories, with every risk category containing a large (&gt;50%) proportion of the lowest risk level (level 1). This is despite weighting the models to account for class imbalance.</p>
<p><strong>Validation peformance:</strong></p>
<p><img src="images/long-stay-baseline/clf-predicted-actuals-validation.png" alt="Plots of predicted vs actual risks on the validation dataset"/></p>
<p>Figure 25. Plots of predicted vs actual risks on the validation dataset. Left image shows count of actual and predicted risks for each category. Right image shows proportion of actual risk that makes up each predicted risk category.</p>
<p>The RandomForest model has an anomaly in its predictions for risk category 4 where it is missing any of the highest risk category 5 compared to other predictions. This is likely due to the overfitting observed in the previous plot.</p>
<p>Both CatBoost and XGBoost have similar levels of predictive power, defined by the lower proportion of very low risk in the predictions for high risk, although at ~50% these are still too high.</p>
<p>Both CatBoost and XGBoost overpredict higher risk categories, while underpredicting the lowest risk category. This will lead both to false positives where very low risk cases are shown as high risk, and false negatives where high risk cases are shown as lower risk.</p>
<p>CatBoost was selected as the final model due to the lack of significant difference in performance with XGBoost, and for consistency with the final regression model. Further tuning of the CatBoost model using GridSearch (with a smaller paramater space than with regression due to compute time) and cross-validation led to the following results:</p>
<table><thead><tr><th>Parameter</th><th>Optimal value</th></tr></thead><tbody><tr><td><code>depth</code></td><td>10</td></tr><tr><td><code>l2_leaf_reg</code></td><td>1</td></tr><tr><td><code>learning_rate</code></td><td>0.1</td></tr></tbody></table>
<p>with</p>
<table><thead><tr><th>Model</th><th>Training weighted F1 score</th><th>Validation weighted F1 score</th><th>Test weighted F1 score</th><th>Test balanced accuracy</th><th>Test AUC (OVR, weighted)</th></tr></thead><tbody><tr><td>CatBoost (tuned)</td><td>0.61</td><td>0.75</td><td>0.60</td><td>0.27</td><td>0.70</td></tr></tbody></table>
<p>Balanced accuracy was determined as 0.27, a poor result for accurately predicting the correct class. The overall Area Under the receiving operator characterstic Curve (AUC), which was calculated as a weighted one-versus-rest metric, was 0.70.</p>
<p><img src="images/long-stay-baseline/clf-predicted-actuals-final-model-test.png" alt="Plots of predicted vs actual for the final model - test set"/></p>
<p>Figure 26. Plots of predicted vs actual risks on the test dataset for the final model. Left image shows count of actual and predicted risks for each category. Right image shows proportion of actual risk that makes up each predicted risk category.</p>
<p>The final model still assigns over 50% of the lower risk class (the most populated class) to every predicted class, which would lead to a high number of false positives. It also fails to capture the highest risk class adequately, leading to a high number of false negatives.</p>
<p>Despite the poor performance, we can still explore the most important features that make up the prediction by plotting feature importances of the final model:</p>
<p><img src="images/long-stay-baseline/clf-feature-importance.png" alt="Feature importances the final model"/></p>
<p>Figure 27. Feature importances for the final regression model.</p>
<p>In this case, <code>arrival_month_name</code> and <code>arrival_day_of_week</code> are the two most important features, which differs from the regression model and correlation analysis. This may be why the false positive and false negative rates for the model are so high, and needs further exploration.</p>
<p>Demographic analysis of the risk stratification model was not conducted as the model performance did not justify exploring whether there was representation bias at this stage.</p>
<h3>6.4 Model comparison<a name="modelcomparison"></a></h3>
<p>As a final modelling step, we can compare both the regression models and classification models, by encoding the predicted length of stay from the regression model as a corresponding risk.</p>
<p>This comparison may help us understand whether a classification or regression approach is more suitable for this type of data.</p>
<p><img src="images/long-stay-baseline/model-comparison.png" alt="Comparison of both models"/></p>
<p>Figure 28. Comparison of both models. Left image shows proportion of actual risk for each predicted risk category for the classification model. Right image shows proportion of actual risk for each equivalent predicted risk category derived from the regression model.</p>
<p>Here we can see that the regression model, encoded as a risk stratification model, performs much better than the classification approach:</p>
<ul>
<li>The number of very low risk patients is much lower for higher risk patients, under 20% in the case of high risk. This means lower false positives.</li>
<li>The proportion of high risk patients is higher in the predicted higher risk categories. This means lower false negatives.</li>
</ul>
<p>If risk stratification is the key desired output, then further refining the regression model may be the better approach to improving the overall performance of the system.</p>
<h2>7. Conclusions<a name="conclusions"></a></h2>
<p>A number of baseline machine learning models were trained on EPR data from GHNHSFT.</p>
<p>The best performing regression model achieved a Mean Absolute Error of 4.1 days, compared to 3.8 days for previous work using a convolutional neural network.</p>
<p>Simpler baseline models benefit from enhanced explainability and less compute resources for training. In this case the most important features were related to age and serious illness.</p>
<p>The overall performance of the best regression model was still poor - despite an MAE of 4.1 days, the model failed to capture long stayers and requires further work before use.</p>
<p>The best performing classification model achieved a weighted F1 score of 0.6.</p>
<p>The overall performance of the best classification model was poor - the model failed to capture high risk and assigned a high proportion (&gt;50%) of very low risk patients to higher risk groups.</p>
<p>Using the regression model to calculate equivalent risk scores led to a better risk stratification model, where only ~20% of very low risk patients were assigned to the high risk group.</p>
<p>Demographic analysis showed that the model performed differently for different ethnicities and indices of multiple deprivation, but both model performance needs to be improved and sample sizes need to be increased in order to draw any meaning from these initial findings.</p>
<p>There is opportunity for much future work, which should be balanced with the utility of these predictions in the clinical context.</p>
<h2>8. Future work<a name="futurework"></a></h2>
<h3>Modelling improvements</h3>
<ol>
<li>Feature engineering of free text fields. Early on we decided to focus on simple numerical and categorical features for this project. A huge amount of rich data is present in fields such as <code>presenting_complaint</code> and <code>reason_for_admission</code>.</li>
<li>Including features available after admission. Fields such as <code>all_diagnoses</code> and <code>all_treatments</code> will provide clinically important information, and may improve the performance of the predictions.</li>
<li>Focussing on a smaller number of features. Once the most important features are identified, a model using the top e.g. 10 features could be trained and tested.</li>
<li>Building two models - one for short stay and one for long stay. This may help capture the bimodal nature of the underlying dataset.</li>
<li>Including <code>MINOR</code> cases. This project focussed on <code>MAJOR</code>, <code>non-elective</code> cases. 70%+ of the original data belonged to minor cases, and in combination with the above, including this data could lead to an improvement in model performance.</li>
<li>Treating Length of Stay as a discrete variable and applying poisson distribution appropriate approaches to modelling.</li>
<li>Exploring Generalised Linear Models using e.g. <a href="https://pygam.readthedocs.io/en/latest/index.html">pyGAM</a>.</li>
<li>Exploring Bayesian approaches.</li>
<li>Exploring the addition of latent variable(s).</li>
</ol>
<h3>Demographic analysis improvements</h3>
<ol>
<li>Statistical testing of fairness. Once model performance reaches a sufficient level, further statistical tests of model performance across demographics should be conducted using e.g. a two-sided Kolmogorov-Smirnov test.</li>
<li>Combine smaller groups. For example, grouping <code>British</code> and <code>Non-British</code> ethnicities would allow statistical comparisons to be made between the majority group and other groups.</li>
</ol>
<h3>Technical improvements</h3>
<ol>
<li>Move from Notebooks to python scripts. Jupyter Notebooks are an excellent exploratory tool, but do not work well with version control or automated testing.</li>
<li>Implement a <a href="https://github.com/NHSDigital/rap-community-of-practice">Reproducible Analytical Pipeline</a>. This will allow reuse of the approaches here and improve overall code quality.</li>
<li>Abstract visualisation code into functions. This will improve readability of the code.</li>
</ol>
<h4>Acknowledgments</h4>
<ol>
<li>Joe Green, GHNHSFT for presenting the challenge to Skunkworks and supporting problem definition/data selection</li>
<li>Tom Lane, GHNHSFT for support in final stages</li>
<li>Brad Pearce and Peter Coetzee, Polygeist, for the original CNN-based model</li>
<li>Jennifer Hall, Matthew Cooper and Sanson Poon, NHS AI Lab Skunkworks for guidance, code and report review</li>
<li>Chris Mainey, NHSE, for suggestions of additional modelling improvements</li>
</ol>
<table><thead><tr><th>Output</th><th>Link</th></tr></thead><tbody><tr><td>Open Source Code &amp; Documentation</td><td><a href="https://github.com/nhsx/skunkworks-long-stayer-risk-stratification-baseline">Github</a></td></tr></tbody></table></div><div class="flex justify-between items-center"></div></div></div></div></main></div></div><div class="bg-gray-200 text-gray-500 border-t-4 border-blue-500 py-8 md:py-10"><div class="mx-auto px-4 sm:px-6 lg:px-8 max-w-6xl"><div class="space-y-10 sm:space-y-6"><div class="flex flex-col space-y-6 md:space-y-0 md:space-x-10 md:flex-row md:justify-between md:items-start"><div class="space-y-2 md:space-y-0 -ml-4"><a class="block md:inline-block underline px-4 pb-4" href="/skunkworks">Overview</a><a class="block md:inline-block underline px-4 pb-4" href="/skunkworks/data-lens">Projects</a><a class="block md:inline-block underline px-4 pb-4" href="/skunkworks/ai-deep-dive">Playbooks</a><a class="block md:inline-block underline px-4 pb-4" href="/skunkworks/casestudy-parkinsons-detection">Case Study Archive</a><a class="block md:inline-block underline px-4 pb-4" href="/skunkworks/team">Team</a></div><div class="flex-shrink-0">© Copyright 2021-2023 NHS AI Lab</div></div><div class="flex flex-col space-y-3 sm:flex-row sm:items-center sm:space-y-0 sm:space-x-3 text-base"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 483.2 195.7" height="17" width="41" focusable="false"><path fill="currentColor" d="M421.5 142.8V.1l-50.7 32.3v161.1h112.4v-50.7zm-122.3-9.6A47.12 47.12 0 0 1 221 97.8c0-26 21.1-47.1 47.1-47.1 16.7 0 31.4 8.7 39.7 21.8l42.7-27.2A97.63 97.63 0 0 0 268.1 0c-36.5 0-68.3 20.1-85.1 49.7A98 98 0 0 0 97.8 0C43.9 0 0 43.9 0 97.8s43.9 97.8 97.8 97.8c36.5 0 68.3-20.1 85.1-49.7a97.76 97.76 0 0 0 149.6 25.4l19.4 22.2h3v-87.8h-80l24.3 27.5zM97.8 145c-26 0-47.1-21.1-47.1-47.1s21.1-47.1 47.1-47.1 47.2 21 47.2 47S123.8 145 97.8 145"></path></svg><span class="">All content is available under the Open Government Licence v3.0, except where otherwise stated.</span></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"source":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, {})\n  })) : _createMdxContent();\n  function _createMdxContent() {\n    const _components = Object.assign({\n      p: \"p\",\n      a: \"a\",\n      h2: \"h2\",\n      ul: \"ul\",\n      li: \"li\",\n      ol: \"ol\",\n      strong: \"strong\",\n      img: \"img\",\n      em: \"em\",\n      code: \"code\",\n      pre: \"pre\",\n      h3: \"h3\",\n      h4: \"h4\",\n      table: \"table\",\n      thead: \"thead\",\n      tr: \"tr\",\n      th: \"th\",\n      tbody: \"tbody\",\n      td: \"td\",\n      blockquote: \"blockquote\"\n    }, _provideComponents(), props.components), {Tags, Alert} = _components;\n    if (!Alert) _missingMdxReference(\"Alert\", true);\n    if (!Tags) _missingMdxReference(\"Tags\", true);\n    return _jsxs(_Fragment, {\n      children: [_jsx(Tags, {\n        title: \"\",\n        tags: ['LoS', 'length of stay', 'baseline', 'risk model', 'regression', 'classification']\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Long Stayer risk stratification baseline models was selected as a project to run in tandem with the \", _jsx(_components.a, {\n          href: \"long-stay\",\n          children: \"Long Stayer Risk Stratification\"\n        }), \" project, and started in March 2022.\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Baseline models provide a mechanism to generate baseline metrics to assess the performance of more complex models, and establish the effectiveness of simple approaches.\"\n      }), \"\\n\", _jsx(Alert, {\n        title: \"Intended audience\",\n        children: \"This report has been written for analysts and data scientists at NHS Trusts/ALBs\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"A series of Jupyter Notebooks used to generate this report are available on \", _jsx(_components.a, {\n          href: \"https://github.com/nhsx/skunkworks-long-stayer-risk-stratification-baseline/tree/main/notebooks\",\n          children: \"Github\"\n        }), \".\"]\n      }), \"\\n\", _jsx(_components.h2, {\n        children: \"Table of contents\"\n      }), \"\\n\", _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.ol, {\n            children: [\"\\n\", _jsx(_components.li, {\n              children: _jsx(_components.a, {\n                href: \"#background\",\n                children: \"Background\"\n              })\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.ol, {\n            start: \"2\",\n            children: [\"\\n\", _jsx(_components.li, {\n              children: _jsx(_components.a, {\n                href: \"#approach\",\n                children: \"Approach\"\n              })\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.ol, {\n            start: \"3\",\n            children: [\"\\n\", _jsx(_components.li, {\n              children: _jsx(_components.a, {\n                href: \"#dataingestandprocessing\",\n                children: \"Data ingest and processing\"\n              })\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.ol, {\n            start: \"4\",\n            children: [\"\\n\", _jsx(_components.li, {\n              children: _jsx(_components.a, {\n                href: \"#featureengineering\",\n                children: \"Feature engineering\"\n              })\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.ol, {\n            start: \"5\",\n            children: [\"\\n\", _jsx(_components.li, {\n              children: _jsx(_components.a, {\n                href: \"#statisticalanalysis\",\n                children: \"Statistical analysis\"\n              })\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.ol, {\n            start: \"6\",\n            children: [\"\\n\", _jsx(_components.li, {\n              children: _jsx(_components.a, {\n                href: \"#modelling\",\n                children: \"Modelling\"\n              })\n            }), \"\\n\"]\n          }), \"\\n\", _jsxs(_components.ul, {\n            children: [\"\\n\", _jsxs(_components.li, {\n              children: [\"6.1 \", _jsx(_components.a, {\n                href: \"#regressionmodels\",\n                children: \"Regression models\"\n              })]\n            }), \"\\n\", _jsxs(_components.li, {\n              children: [\"6.2 \", _jsx(_components.a, {\n                href: \"#demographicanalysis\",\n                children: \"Demographic analysis\"\n              })]\n            }), \"\\n\", _jsxs(_components.li, {\n              children: [\"6.3 \", _jsx(_components.a, {\n                href: \"#classificationmodels\",\n                children: \"Classification models\"\n              })]\n            }), \"\\n\", _jsxs(_components.li, {\n              children: [\"6.4 \", _jsx(_components.a, {\n                href: \"#modelcomparison\",\n                children: \"Model comparison\"\n              })]\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.ol, {\n            start: \"7\",\n            children: [\"\\n\", _jsx(_components.li, {\n              children: _jsx(_components.a, {\n                href: \"#conclusions\",\n                children: \"Conclusions\"\n              })\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"\\n\", _jsxs(_components.ol, {\n            start: \"8\",\n            children: [\"\\n\", _jsx(_components.li, {\n              children: _jsx(_components.a, {\n                href: \"#futurework\",\n                children: \"Future work\"\n              })\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.h2, {\n        children: [\"1. Background\", _jsx(\"a\", {\n          name: \"background\"\n        })]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Hospital long stayers, those with a \", _jsx(\"a\", {\n          href: \"https://www.england.nhs.uk/urgent-emergency-care/reducing-length-of-stay/\",\n          target: \"_blank\",\n          children: \"length of stay (LoS) of 21 days or longer\"\n        }), \", have significantly worse medical and social outcomes than other patients. Long-stayers are often medically optimised (fit for discharge) many days before their actual discharge. Moreover, there are a complex mixture of medical, cultural and socioeconomic factors which contribute to the causes of unnecessary long stays.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"This project aims to complement \", _jsx(_components.a, {\n          href: \"long-stay\",\n          children: \"previous work\"\n        }), \" by generating simple baseline regression and classification models that could be replicated at other hospital trusts, and is divided into two phases:\"]\n      }), \"\\n\", _jsxs(_components.ol, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"Series of Jupyter Notebooks containing baseline model code\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.a, {\n            href: \"https://github.com/NHSDigital/rap-community-of-practice\",\n            children: \"Reproducible Analytical Pipeline\"\n          }), \" including data pipelines\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Currently, this project has completed \", _jsx(_components.strong, {\n          children: \"Phase 1\"\n        }), \".\"]\n      }), \"\\n\", _jsxs(_components.h2, {\n        children: [\"2. Approach\", _jsx(\"a\", {\n          name: \"approach\"\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The aim of this project is to perform the simplest possible feature engineering and modelling to arrive at a reasonable baseline model, for more advanced feature engineering and modelling to be compared against.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The approach involved:\"\n      }), \"\\n\", _jsxs(_components.ol, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"Defining the population and data cleaning\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Feature engineering, focussing on basic numerical and categorical features\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Simple baseline models implemented using commonly available packages including \", _jsx(_components.a, {\n            href: \"https://scikit-learn.org/\",\n            children: \"scikit-learn 1.1.1\"\n          }), \", \", _jsx(_components.a, {\n            href: \"https://catboost.ai\",\n            children: \"CatBoost 1.0.6\"\n          }), \" and \", _jsx(_components.a, {\n            href: \"https://xgboost.readthedocs.io/en/stable/\",\n            children: \"XGBoost 1.3.3\"\n          })]\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Analysis of model performance by demographic\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"A comparison of regression-based and classification-based risk stratification models\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"A set of extensions for future work\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.h2, {\n        children: [\"3. Data ingest and processing\", _jsx(\"a\", {\n          name: \"dataingestandprocessing\"\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"GHNHSFT performed a SQL export from their EPR system containing ~770,000 records across 99 columns, with significant sparsity across several columns and a section of rows, as visualised by light coloured blocks (null values) in the below image:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/sparsity.png\",\n          alt: \"Image of data sparsity\"\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Figure 1. Plot of data sparsity of raw data. Null values are light coloured blocks. (\", _jsx(_components.em, {\n          children: \"Note that not all columns are labelled\"\n        }), \")\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"The population for this study was defined as non-elective, major cases as recorded in the \", _jsx(_components.code, {\n          children: \"IS_MAJOR\"\n        }), \" and \", _jsx(_components.code, {\n          children: \"elective_or_non_elective\"\n        }), \" fields.\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Filtering the dataset to this definition resulted in a reduction of ~770,000 rows to ~170,000 rows (78% reduction):\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/sparsity-major.png\",\n          alt: \"Image of data sparsity - major cases\"\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Figure 2. Plot of data sparsity of \\\"major\\\" cases. Null values are light coloured blocks. (\", _jsx(_components.em, {\n          children: \"Note that not all columns are labelled\"\n        }), \")\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Data was processed by:\"\n      }), \"\\n\", _jsxs(_components.ol, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"Converting datetime columns into the correct data type\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Ordering records by \", _jsx(_components.code, {\n            children: \"START_DATE_TIME_HOSPITAL_PROVIDER_SPELL\"\n          })]\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Removing fields not available at admission\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Removing empty and redundant (e.g. \", _jsx(_components.code, {\n            children: \"LENGTH_OF_STAY_IN_MINUTES\"\n          }), \" duplicates \", _jsx(_components.code, {\n            children: \"LENGTH_OF_STAY\"\n          }), \") columns\"]\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Removing duplicate rows\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Removing local identifiers\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Imputing \", _jsx(_components.code, {\n            children: \"stroke_ward_stay\"\n          }), \" as \", _jsx(_components.code, {\n            children: \"N\"\n          }), \" if not specified\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Binary encoding \", _jsx(_components.code, {\n            children: \"stroke_ward_stay\"\n          }), \", \", _jsx(_components.code, {\n            children: \"IS_care_home_on_admission\"\n          }), \", \", _jsx(_components.code, {\n            children: \"IS_care_home_on_discharge\"\n          }), \" and \", _jsx(_components.code, {\n            children: \"IS_illness_not_injury\"\n          })]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"This resulted in ~170,000 rows across ~50 columns as visualised in the below image:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/sparsity-clean.png\",\n          alt: \"Image of data sparsity of \\\"clean\\\" data\"\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Figure 3. Plot of data sparsity of clean data. Null values are light coloured blocks. (\", _jsx(_components.em, {\n          children: \"Note that not all columns are labelled\"\n        }), \")\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"The resulting data dictionary is available \", _jsx(_components.a, {\n          href: \"https://github.com/nhsx/skunkworks-long-stayer-risk-stratification-baseline/blob/main/docs/data-dictionary.csv\",\n          children: \"here\"\n        }), \".\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Additionally, Length of Stay was capped to 30 days, due to a long tail of long stayers over ~15 days and the definition of long stayer being over 21 days. The effect of capping can be visualised by comparing box plots of the distribution of length of stay on the raw data (left image, y scale up to 300 days) and the capped data (right image, y scale up to 30 days):\"\n      }), \"\\n\", _jsx(\"p\", {\n        align: \"center\",\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/los-boxplot.png\",\n          alt: \"Boxplot of length of stay\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 4. Plot of the distribution of long stayers in the raw (left) data and capped (right) data. Note different y scales.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The resulting distribution of length of stays shows a ~bimodal distribution caused by the capping - the majority of stays are short (\u003c5 days), which a long tail and population of long stayers:\"\n      }), \"\\n\", _jsx(\"p\", {\n        align: \"center\",\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/los-density.png\",\n          alt: \"Density plot of length of stay\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 5. Plot of density of length of stay for capped data.\"\n      }), \"\\n\", _jsxs(_components.h2, {\n        children: [\"4. Feature engineering\", _jsx(\"a\", {\n          name: \"featureengineering\"\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"After discussion with GHNHSFT, the following decisions were made in feature selection:\"\n      }), \"\\n\", _jsxs(_components.ol, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"Select the following features for inclusion in the model, which are available on admission:\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          children: \"    \\\"ae_arrival_mode\\\",\\n    \\\"AGE_ON_ADMISSION\\\",\\n    \\\"EL CountLast12m\\\",\\n    \\\"EMCountLast12m\\\",\\n    \\\"IS_illness_not_injury\\\",\\n    \\\"IS_cancer\\\",\\n    \\\"IS_care_home_on_admission\\\",\\n    \\\"IS_chronic_kidney_disease\\\",\\n    \\\"IS_COPD\\\",\\n    \\\"IS_coronary_heart_disease\\\",\\n    \\\"IS_dementia\\\",\\n    \\\"IS_diabetes\\\",\\n    \\\"IS_frailty_proxy\\\",\\n    \\\"IS_hypertension\\\",\\n    \\\"IS_mental_health\\\",\\n    \\\"MAIN_SPECIALTY_CODE_AT_ADMISSION_DESCRIPTION\\\",\\n    \\\"OP First CountLast12m\\\",\\n    \\\"OP FU CountLast12m\\\",\\n    \\\"SOURCE_OF_ADMISSION_HOSPITAL_PROVIDER_SPELL_DESCRIPTION\\\",\\n    \\\"stroke_ward_stay\\\",\\n    \\\"LENGTH_OF_STAY\\\",\\n    \\\"arrival_day_of_week\\\",\\n    \\\"arrival_month_name\\\"\\n\"\n        })\n      }), \"\\n\", _jsxs(_components.ol, {\n        start: \"2\",\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"Exclude the following features, but retain for later analysis of model fairness:\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          className: \"language-python\",\n          children: \"    \\\"ETHNIC_CATEGORY_CODE_DESCRIPTION\\\",\\n    \\\"IMD county decile\\\",\\n    \\\"OAC Group Name\\\",\\n    \\\"OAC Subgroup Name\\\",\\n    \\\"OAC Supergroup Name\\\",\\n    \\\"PATIENT_GENDER_CURRENT_DESCRIPTION\\\",\\n    \\\"POST_CODE_AT_ADMISSION_DATE_DISTRICT\\\",\\n    \\\"Rural urban classification\\\"\\n\"\n        })\n      }), \"\\n\", _jsxs(_components.ol, {\n        start: \"3\",\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"Generate \", _jsx(_components.code, {\n            children: \"arrival_day_of_week\"\n          }), \" and \", _jsx(_components.code, {\n            children: \"arrival_month_name\"\n          }), \" recalculated from \", _jsx(_components.code, {\n            children: \"START_DATE_TIME_HOSPITAL_PROVIDER_SPELL\"\n          })]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"This resulted in a dataset of \", _jsx(_components.strong, {\n          children: \"~170,000 rows across 30 columns\"\n        }), \".\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"One-hot encoding was performed for categorical variables, but non-one-hot encoded features were also kept for models like CatBoost which \", _jsx(_components.a, {\n          href: \"https://catboost.ai/en/docs/features/categorical-features\",\n          children: \"manage categorical variables themselves\"\n        }), \".\"]\n      }), \"\\n\", _jsxs(_components.h3, {\n        children: [\"5. Statistical analysis\", _jsx(\"a\", {\n          name: \"statisticalanalysis\"\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"In order to select appropriate modelling approaches, some basic statistical analysis was conducted to understand normality and inter-correlation of the selected features.\"\n      }), \"\\n\", _jsx(_components.h4, {\n        children: \"Correlation analysis\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Correlation analysis confirmed presence of significant \", _jsx(_components.strong, {\n          children: \"collinearity\"\n        }), \" between different features.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Top 20 one-hot encoded features correlated with \", _jsx(_components.code, {\n          children: \"LENGTH_OF_STAY\"\n        }), \", ranked by absolute correlation, were:\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/correlation.png\",\n          alt: \"Plot of correlations with LENGTH_OF_STAY\"\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Figure 6. Plot of top 20 correlated features with \", _jsx(_components.code, {\n          children: \"LENGTH_OF_STAY\"\n        }), \". Blue columns are positively correlated (ie. increase length of stay) and red columns are negatively correlated (ie. reduce length of stay).\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"These indicate that age and age-related illness, as well as arrival mode are strong factors in determining length of stay.\"\n      }), \"\\n\", _jsx(_components.h4, {\n        children: \"Variation inflation factors\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Variation inflation factors (VIF) confirmed the presence of multi-colinearity between a number of features (VIF \u003e 10).\"\n      }), \"\\n\", _jsx(_components.h4, {\n        children: \"Homoescadisticity\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"A basic ordinary least squares (OLS) regression model was fitted to the full feature set, then residuals calculated.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Residuals failed Shapiro-Wilk, Kolmogorov-Smirnov and Anderson-Darling tests for normality, as well as visual inspection:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/residuals.jpeg\",\n          alt: \"Plot of residuals for OLS model of length of stay\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 7. Plot of residuals (errors) in an OLS model of length of stay for all data.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"OLS methods were therefore excluded from modelling.\"\n      }), \"\\n\", _jsxs(_components.h2, {\n        children: [\"6. Modelling\", _jsx(\"a\", {\n          name: \"modelling\"\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The machine learning modelling approach was as follows:\"\n      }), \"\\n\", _jsxs(_components.ol, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"Split the data into a training (70%), validation (15%) and test (15%) data set\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Check the data splits do not introduce selection bias for length of stay, age, sex, or ethnicity\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Train baseline models with default parameters on the training set\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Evaluate baseline models on the validation test\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Select the best performing model\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Tune the best performing model using cross-validation on the training and validation set\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Report the final performance of the model using the test set\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/ml-approach.png\",\n          alt: \"Summary of machine learning approach\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 8. Summary of machine learning approach used in this project.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Training, validation and test splits were representative of the population and did not introduce selection bias:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.strong, {\n          children: \"Length of stay\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/split-los.png\",\n          alt: \"Distribution of length of stay by data split\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 9. Distribution of length of stay by data split.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.strong, {\n          children: \"Age\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/split-age.png\",\n          alt: \"Distribution of age by data split\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 10. Distribution of age by data split.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.strong, {\n          children: \"Sex\"\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Proportion of \", _jsx(_components.code, {\n          children: \"male\"\n        }), \", \", _jsx(_components.code, {\n          children: \"female\"\n        }), \" patients in each split:\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          children: \"train: [0.53, 0.47]\\nvalidate: [0.51, 0.49]\\ntest: [0.53, 0.47]\\n\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.strong, {\n          children: \"Ethnicity\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Proportions of each ethnicity for each split:\"\n      }), \"\\n\", _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          children: \"train: [0.87, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.0, ...]\\nvalidate: [0.88, 0.05, 0.03, 0.02, 0.01, 0.01, 0.01, 0.0, ...]\\ntest: [0.87, 0.05, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, ...]\\n\"\n        })\n      }), \"\\n\", _jsxs(_components.h3, {\n        children: [\"6.1 Regression models\", _jsx(\"a\", {\n          name: \"regressionmodels\"\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"A range of baseline regression models were selected:\"\n      }), \"\\n\", _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Model\"\n            }), _jsx(_components.th, {\n              children: \"Rationale\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html\",\n                children: \"Mean\"\n              })\n            }), _jsx(_components.td, {\n              children: \"The simplest baseline, uses the mean length of stay as the prediction in all cases\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\",\n                children: \"ElasticNet\"\n              })\n            }), _jsx(_components.td, {\n              children: \"A regularised implementation of linear regression that can be used for multi-colinear datasets such as in this dataset\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\",\n                children: \"DecisionTreeRegressor\"\n              })\n            }), _jsx(_components.td, {\n              children: \"A simple, single tree regressor that is highly explainable\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\",\n                children: \"RandomForestRegressor\"\n              })\n            }), _jsx(_components.td, {\n              children: \"An ensemble of decision trees with potentially better performance than a single tree\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor\",\n                children: \"XGBRegressor\"\n              })\n            }), _jsx(_components.td, {\n              children: \"A boosted tree technique that can improve on ensemble techniques such as RandomForest\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://catboost.ai/en/docs/concepts/python-reference_catboostregressor\",\n                children: \"CatBoostRegressor\"\n              })\n            }), _jsx(_components.td, {\n              children: \"A boosted tree technique designed specifically for datasets with high levels of categorical features as in this dataset\"\n            })]\n          })]\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Each model was trained with default parameters, and evaluated using root mean squared error (RMSE) on both the training set and then on the (unseen) validation set:\"\n      }), \"\\n\", _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Model\"\n            }), _jsx(_components.th, {\n              children: \"Training RMSE (days)\"\n            }), _jsx(_components.th, {\n              children: \"Validation RMSE (days)\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Mean\"\n            }), _jsx(_components.td, {\n              children: \"6.89\"\n            }), _jsx(_components.td, {\n              children: \"6.94\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"ElasticNet\"\n            }), _jsx(_components.td, {\n              children: \"6.55\"\n            }), _jsx(_components.td, {\n              children: \"6.60\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"DecisionTree\"\n            }), _jsx(_components.td, {\n              children: \"0.55\"\n            }), _jsx(_components.td, {\n              children: \"9.11\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"RandomForest\"\n            }), _jsx(_components.td, {\n              children: \"2.46\"\n            }), _jsx(_components.td, {\n              children: \"6.52\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"XGBoost\"\n            }), _jsx(_components.td, {\n              children: \"5.97\"\n            }), _jsx(_components.td, {\n              children: \"6.32\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"CatBoost\"\n            }), _jsx(_components.td, {\n              children: \"6.13\"\n            }), _jsx(_components.td, {\n              children: \"6.26\"\n            })]\n          })]\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The best performing baseline model was Catboost with an RMSE of 6.26 days. Both DecisionTree and RandomForest models overfit the training data, as seen with low training RMSE resulting in much higher validation RMSE.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"A single metric (e.g. RMSE) does not capture the behaviour of each model, so we visualise both the Predicted vs Actual plots as well as the corresponding relative error for both the training set and the validation set:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.strong, {\n          children: \"Training performance:\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/regression-predicted-actuals-training.png\",\n          alt: \"Plots of predicted vs actual and corresponding errors on the training dataset\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 11. Plots of predicted vs actual (left, red dashed line shows ideal model) and corresponding relative errors (right, red solid line shows mean error with 95% limits of agreement in green dashed lines) on the training dataset.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The RandomForest model appears to fit the training data well, but when compared with the performance on the validation set below, we can see this is due to overfitting on the training data set:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.strong, {\n          children: \"Validation performance:\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/regression-predicted-actuals-validation.jpeg\",\n          alt: \"Plots of predicted vs actual and corresponding errors on the validation dataset\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 12. Plots of predicted vs actual (left, red dashed line shows ideal model) and corresponding relative errors (right, red solid line shows mean error with 95% limits of agreement in green dashed lines) on the validation dataset.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"In all cases, the poor predictive power at higher length of stays is evident - there appears to be a linear increase in error caused by the models' inability to predict higher length of stays.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"This is likely due to the bimodal nature of the underlying length of stay values - most stayers are short, while there is a significant portion of long stayers.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Further tuning of the CatBoost model using GridSearch and cross-validation led to the following results:\"\n      }), \"\\n\", _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Parameter\"\n            }), _jsx(_components.th, {\n              children: \"Optimal value\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.code, {\n                children: \"depth\"\n              })\n            }), _jsx(_components.td, {\n              children: \"6\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.code, {\n                children: \"l2_leaf_reg\"\n              })\n            }), _jsx(_components.td, {\n              children: \"9\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.code, {\n                children: \"learning_rate\"\n              })\n            }), _jsx(_components.td, {\n              children: \"0.1\"\n            })]\n          })]\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"with\"\n      }), \"\\n\", _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Model\"\n            }), _jsx(_components.th, {\n              children: \"Training RMSE (days)\"\n            }), _jsx(_components.th, {\n              children: \"Validation RMSE (days)\"\n            }), _jsx(_components.th, {\n              children: \"Test RMSE (days)\"\n            }), _jsx(_components.th, {\n              children: \"Test MAE (days)\"\n            })]\n          })\n        }), _jsx(_components.tbody, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"CatBoost (tuned)\"\n            }), _jsx(_components.td, {\n              children: \"6.24\"\n            }), _jsx(_components.td, {\n              children: \"6.18\"\n            }), _jsx(_components.td, {\n              children: \"6.06\"\n            }), _jsx(_components.td, {\n              children: \"4.12\"\n            })]\n          })\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The test MAE of 4.12 days compares reasonably well to the previous work using a convolutional neural network which achieved a MAE of 3.8 days.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"However, a plot of predicted vs actual using the test dataset shows again the model's inability to capture long stayers:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/regression-predicted-actuals-final-model-test.jpeg\",\n          alt: \"Plots of predicted vs actual and corresponding errors for the final model - test set\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 13. Plots of predicted vs actual (left, red dashed line shows ideal model) and corresponding relative errors (right, red solid line shows mean error with 95% limits of agreement in green dashed lines) on the test dataset for the final model.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"We can still explore the most important features that make up the prediction by plotting feature importances of the final model:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/regression-feature-importance.png\",\n          alt: \"Feature importances the final model\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 14. Feature importances for the final regression model.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"These broadly align with the correlated features explored earlier on - namely, age, arrival mode, serious illness but also include the number of previous visits, which can be considered a proxy for serious illness itself.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Because the final model, using CatBoost, does not include one-hot encoding of the categorical data as CatBoost deals with this internally, we don't have further granularity on admission mode and arrival mode to compare.\"\n      }), \"\\n\", _jsxs(_components.h3, {\n        children: [\"6.2 Demographic analysis\", _jsx(\"a\", {\n          name: \"demographicanalysis\"\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"While the model is not peformant enough to deploy into production, it is still important to understand whether or not the model incorporates bias into its predictions.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"There are many kinds of bias in machine learning projects, and here we are looking at representation bias:\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"Does the model perform better or worse for specific categories of people across sex, ethnicity and other demographics?\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The specific categories are:\"\n      }), \"\\n\", _jsx(_components.pre, {\n        children: _jsx(_components.code, {\n          children: \"\\\"ETHNIC_CATEGORY_CODE_DESCRIPTION\\\", \\\"IMD county decile\\\", \\\"OAC Group Name\\\", \\\"OAC Subgroup Name\\\", \\\"OAC Supergroup Name\\\", \\\"PATIENT_GENDER_CURRENT_DESCRIPTION\\\", \\\"POST_CODE_AT_ADMISSION_DATE_DISTRICT\\\", \\\"Rural urban classification\\\"\\n\"\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Before looking at model performance, we need to understand how represented each category is, before drawing conclusions on categories with small sample size (note that for brevity we will only share results from \", _jsx(_components.code, {\n          children: \"\\\"ETHNIC_CATEGORY_CODE_DESCRIPTION\\\", \\\"IMD county decile\\\",\\\"PATIENT_GENDER_CURRENT_DESCRIPTION\\\"\"\n        }), \"):\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/los-dist-ethnicity.png\",\n          alt: \"Underlying counts for ethnicity - all data\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 15. Underlying counts for ethnicity - all data.\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"We can see that for \", _jsx(_components.code, {\n          children: \"ETHNIC_CATEGORY_CODE_DESCRIPTION\"\n        }), \", the overwhelming majority of patients report \", _jsx(_components.code, {\n          children: \"British\"\n        }), \". We should be careful what conclusions we draw in further analysis about smaller categories, as the sample size will be very small and likely not statistically representative.\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/los-dist-sex.png\",\n          alt: \"Underlying counts for sex - all data\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 16. Underlying counts for sex - all data.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Sex is broadly equal, with slightly more female than male patients in this dataset.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/los-dist-imd.png\",\n          alt: \"Underlying counts for index of multiple deprivation - all data\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 17. Underlying counts for index of multiple deprivation - all data\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Index of Multiple Deprivation (IMD) deciles are skewed to the lower end, ie. there are more deprived patients present in this dataset than not.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Now we can look at the distribution of length of stay for the above categories:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/los-mean-los-ethnicity.png\",\n          alt: \"Underlying length of stay by ethnicity - all data\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 18. Underlying length of stay by ethnicity - all data.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"There is significant variation of length of stay for different ethnic groups, for example with White and black Carribean patients having a length of stay of 2.6 days on average, versus 6.0 days for Irish patients. However, as discussed previously, the count of these groups is 560 and 892 individuals respectively so further statistical hypothesis tests need to be conducted to understand if the distributions are truly different (e.g. a two-sided Kolmogorov-Smirnov test).\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/los-mean-los-sex.png\",\n          alt: \"Underlying length of stay by sex - all data\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 19. Underlying length of stay by sex - all data.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Mean length of stay is almost identical across patient sex.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/los-mean-los-imd.png\",\n          alt: \"Underlying length of stay by index of multiple deprivation - all data\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 20. Underlying length of stay by index of multiple deprivation - all data.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"There are small variations in length of stay across IMD deciles, although more tests need to be conducted to understand if these differences are statistically significant.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Because we are interested in if the model performs differently by category, we will plot the error of the predictions of the test dataset relative to the overall (mean) error for all categories. This will help identify potential discrimination in model performance.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/los-rel-error-ethnicity.png\",\n          alt: \"Relative error in length of stay predictions for different ethnic groups - test data\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 21. Relative error in length of stay predictions for different ethnic groups - test data.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The model appears to perform significantly worse for Carribean (overestimating length of stay by 2.7 days compared to the mean error) and Any other mixed background (underestimating length of stay by 1.8 days compared to the mean error). Sample sizes are 719 and 536 patients respectively. As discussed the small sample sizes need further investigation and/or additional data collection to establish the statistical significance of this performance difference.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/los-rel-error-sex.png\",\n          alt: \"Relative error in length of stay predictions for different sex - test data\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 22. Relative error in length of stay predictions for different sex - test data.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Sex has almost no (0.002 days) error from the average.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/los-rel-error-imd.png\",\n          alt: \"Relative error in length of stay predictions for different index of multiple deprivations deciles - test data\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 23. Relative error in length of stay predictions for different index of multiple deprivations deciles - test data.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The lowest IMD county decile (1) has an error of 0.5 days underestimating from the mean error, which at under a day may not lead to any difference in treatment if this prediction is used in clinical practice (ie. a length of stay of 1.5 days is the same as a length of stay of 2.0 days - both would count as 2 whole days).\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"We also know that length of stay varies by group, so further plots of the ratio of MAE to length of stay are generated in the notebooks, but not included here for brevity.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The final model generated did not adequately capture length of stay across the population. Some sample sizes of demographic groups were too small to draw conclusions, but the process of exploring the underlying distribution of the target feature (length of stay), count (n) and model performance were important and should remain part of future work.\"\n      }), \"\\n\", _jsxs(_components.h3, {\n        children: [\"6.3 Classification models\", _jsx(\"a\", {\n          name: \"classificationmodels\"\n        })]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"In addition to predicting the length of stay in days, we are also interested in stratifying the risk of a patient becoming a long stayer. This can be inferred from their predicted length of stay (see \", _jsx(_components.a, {\n          href: \"#modelcomparison\",\n          children: \"model comparison\"\n        }), \"), but we can also train a classification model to do this directly.\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The agreed stratification of risk of long stay is defined as:\"\n      }), \"\\n\", _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Risk Category\"\n            }), _jsx(_components.th, {\n              children: \"Day Range for Risk Category\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"1 - Very low risk\"\n            }), _jsx(_components.td, {\n              children: \"0-6\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"2 - Low risk\"\n            }), _jsx(_components.td, {\n              children: \"7-10\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"3 - Normal risk\"\n            }), _jsx(_components.td, {\n              children: \"11-13\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"4 - Elevated risk\"\n            }), _jsx(_components.td, {\n              children: \"14-15\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"5 - High risk\"\n            }), _jsx(_components.td, {\n              children: \"\u003e15\"\n            })]\n          })]\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"We keep the training features the same as in the regression models, and encode risk from the actual length of stay as the target feature.\"\n      }), \"\\n\", _jsxs(_components.blockquote, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"Postcript: classification models based on increasing risk (1-5) are ordinal in nature, and an appropriate model should be used where different classes are not treated as independent as per the examples in this implementation.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The classification equivalents of the regression models were selected:\"\n      }), \"\\n\", _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Model\"\n            }), _jsx(_components.th, {\n              children: \"Regression version\"\n            }), _jsx(_components.th, {\n              children: \"Classification version\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Dummy\"\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html\",\n                children: \"Mean\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html\",\n                children: \"Prior\"\n              })\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"ElasticNet\"\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\",\n                children: \"ElasticNet\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\",\n                children: \"LogisticRegression\"\n              })\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Decision Tree\"\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\",\n                children: \"DecisionTreeRegressor\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\",\n                children: \"DecisionTreeClassifier\"\n              })\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Random Forest\"\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\",\n                children: \"RandomForestRegressor\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\",\n                children: \"RandomForestClassifier\"\n              })\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"XGBoost\"\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor\",\n                children: \"XGBRegressor\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier\",\n                children: \"XGBClassifier\"\n              })\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"CatBoost\"\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://catboost.ai/en/docs/concepts/python-reference_catboostregressor\",\n                children: \"CatBoostRegressor\"\n              })\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier\",\n                children: \"CatBoostClassifier\"\n              })\n            })]\n          })]\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The training, validation and test regime was the same as for the regression models.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.strong, {\n          children: \"Class imbalance\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"In the training set, we observe the following class imbalance:\"\n      }), \"\\n\", _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Risk score\"\n            }), _jsx(_components.th, {\n              children: \"Number of patients\"\n            }), _jsx(_components.th, {\n              children: \"% of total patients\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"1\"\n            }), _jsx(_components.td, {\n              children: \"89711\"\n            }), _jsx(_components.td, {\n              children: \"74.0\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"2\"\n            }), _jsx(_components.td, {\n              children: \"12634\"\n            }), _jsx(_components.td, {\n              children: \"10.4\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"3\"\n            }), _jsx(_components.td, {\n              children: \"5226\"\n            }), _jsx(_components.td, {\n              children: \"9.1\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"4\"\n            }), _jsx(_components.td, {\n              children: \"2613\"\n            }), _jsx(_components.td, {\n              children: \"4.3\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"5\"\n            }), _jsx(_components.td, {\n              children: \"10990\"\n            }), _jsx(_components.td, {\n              children: \"2.2\"\n            })]\n          })]\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"ie. the majority of patients are low risk, and the highest risk group is only 2.2% of the population.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Class and/or sample weights were calculated using the above training imbalances and passed into all models.\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Models were trained using default parameters, and evaluated using the \", _jsx(_components.a, {\n          href: \"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\",\n          children: \"weighted F1 score\"\n        }), \" which represents the balance between precision and recall, and accounts for class imbalance. F1 scores range from 0 to 1 (where 1 is \\\"ideal\\\" or maximum).\"]\n      }), \"\\n\", _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Model\"\n            }), _jsx(_components.th, {\n              children: \"Training weighted F1 score\"\n            }), _jsx(_components.th, {\n              children: \"Validation weighted F1 score\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Prior\"\n            }), _jsx(_components.td, {\n              children: \"0.63\"\n            }), _jsx(_components.td, {\n              children: \"0.62\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"ElasticNet\"\n            }), _jsx(_components.td, {\n              children: \"0.54\"\n            }), _jsx(_components.td, {\n              children: \"0.53\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"DecisionTree\"\n            }), _jsx(_components.td, {\n              children: \"1.00\"\n            }), _jsx(_components.td, {\n              children: \"0.59\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"RandomForest\"\n            }), _jsx(_components.td, {\n              children: \"1.00\"\n            }), _jsx(_components.td, {\n              children: \"0.64\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"XGBoost\"\n            }), _jsx(_components.td, {\n              children: \"0.57\"\n            }), _jsx(_components.td, {\n              children: \"0.54\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"CatBoost\"\n            }), _jsx(_components.td, {\n              children: \"0.57\"\n            }), _jsx(_components.td, {\n              children: \"0.54\"\n            })]\n          })]\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"While the RandomForest model obtained the highest validation weighted F1 score (0.64), it also overfit the training data (weighted F1 score of 1.00).\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"A visual inspection of model performance, plotting both total counts of risk categories in actual vs predicted cases, as well as the proportion of actual risk in each predicted category, are shown below for both the training and validation data sets:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.strong, {\n          children: \"Training performance:\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/clf-predicted-actuals-training.png\",\n          alt: \"Plots of predicted vs actual risks on the training dataset\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 24. Plots of predicted vs actual risks on the training dataset. Left image shows count of actual and predicted risks for each category. Right image shows proportion of actual risk that makes up each predicted risk category.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"We can see that both the DecisionTree and RandomForest models severely overfit the training data.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"We also see that none of the models are able to capture the nature of the highest risk categories, with every risk category containing a large (\u003e50%) proportion of the lowest risk level (level 1). This is despite weighting the models to account for class imbalance.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.strong, {\n          children: \"Validation peformance:\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/clf-predicted-actuals-validation.png\",\n          alt: \"Plots of predicted vs actual risks on the validation dataset\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 25. Plots of predicted vs actual risks on the validation dataset. Left image shows count of actual and predicted risks for each category. Right image shows proportion of actual risk that makes up each predicted risk category.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The RandomForest model has an anomaly in its predictions for risk category 4 where it is missing any of the highest risk category 5 compared to other predictions. This is likely due to the overfitting observed in the previous plot.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Both CatBoost and XGBoost have similar levels of predictive power, defined by the lower proportion of very low risk in the predictions for high risk, although at ~50% these are still too high.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Both CatBoost and XGBoost overpredict higher risk categories, while underpredicting the lowest risk category. This will lead both to false positives where very low risk cases are shown as high risk, and false negatives where high risk cases are shown as lower risk.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"CatBoost was selected as the final model due to the lack of significant difference in performance with XGBoost, and for consistency with the final regression model. Further tuning of the CatBoost model using GridSearch (with a smaller paramater space than with regression due to compute time) and cross-validation led to the following results:\"\n      }), \"\\n\", _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Parameter\"\n            }), _jsx(_components.th, {\n              children: \"Optimal value\"\n            })]\n          })\n        }), _jsxs(_components.tbody, {\n          children: [_jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.code, {\n                children: \"depth\"\n              })\n            }), _jsx(_components.td, {\n              children: \"10\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.code, {\n                children: \"l2_leaf_reg\"\n              })\n            }), _jsx(_components.td, {\n              children: \"1\"\n            })]\n          }), _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: _jsx(_components.code, {\n                children: \"learning_rate\"\n              })\n            }), _jsx(_components.td, {\n              children: \"0.1\"\n            })]\n          })]\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"with\"\n      }), \"\\n\", _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Model\"\n            }), _jsx(_components.th, {\n              children: \"Training weighted F1 score\"\n            }), _jsx(_components.th, {\n              children: \"Validation weighted F1 score\"\n            }), _jsx(_components.th, {\n              children: \"Test weighted F1 score\"\n            }), _jsx(_components.th, {\n              children: \"Test balanced accuracy\"\n            }), _jsx(_components.th, {\n              children: \"Test AUC (OVR, weighted)\"\n            })]\n          })\n        }), _jsx(_components.tbody, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"CatBoost (tuned)\"\n            }), _jsx(_components.td, {\n              children: \"0.61\"\n            }), _jsx(_components.td, {\n              children: \"0.75\"\n            }), _jsx(_components.td, {\n              children: \"0.60\"\n            }), _jsx(_components.td, {\n              children: \"0.27\"\n            }), _jsx(_components.td, {\n              children: \"0.70\"\n            })]\n          })\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Balanced accuracy was determined as 0.27, a poor result for accurately predicting the correct class. The overall Area Under the receiving operator characterstic Curve (AUC), which was calculated as a weighted one-versus-rest metric, was 0.70.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/clf-predicted-actuals-final-model-test.png\",\n          alt: \"Plots of predicted vs actual for the final model - test set\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 26. Plots of predicted vs actual risks on the test dataset for the final model. Left image shows count of actual and predicted risks for each category. Right image shows proportion of actual risk that makes up each predicted risk category.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The final model still assigns over 50% of the lower risk class (the most populated class) to every predicted class, which would lead to a high number of false positives. It also fails to capture the highest risk class adequately, leading to a high number of false negatives.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Despite the poor performance, we can still explore the most important features that make up the prediction by plotting feature importances of the final model:\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/clf-feature-importance.png\",\n          alt: \"Feature importances the final model\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 27. Feature importances for the final regression model.\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"In this case, \", _jsx(_components.code, {\n          children: \"arrival_month_name\"\n        }), \" and \", _jsx(_components.code, {\n          children: \"arrival_day_of_week\"\n        }), \" are the two most important features, which differs from the regression model and correlation analysis. This may be why the false positive and false negative rates for the model are so high, and needs further exploration.\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Demographic analysis of the risk stratification model was not conducted as the model performance did not justify exploring whether there was representation bias at this stage.\"\n      }), \"\\n\", _jsxs(_components.h3, {\n        children: [\"6.4 Model comparison\", _jsx(\"a\", {\n          name: \"modelcomparison\"\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"As a final modelling step, we can compare both the regression models and classification models, by encoding the predicted length of stay from the regression model as a corresponding risk.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"This comparison may help us understand whether a classification or regression approach is more suitable for this type of data.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsx(_components.img, {\n          src: \"images/long-stay-baseline/model-comparison.png\",\n          alt: \"Comparison of both models\"\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Figure 28. Comparison of both models. Left image shows proportion of actual risk for each predicted risk category for the classification model. Right image shows proportion of actual risk for each equivalent predicted risk category derived from the regression model.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Here we can see that the regression model, encoded as a risk stratification model, performs much better than the classification approach:\"\n      }), \"\\n\", _jsxs(_components.ul, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"The number of very low risk patients is much lower for higher risk patients, under 20% in the case of high risk. This means lower false positives.\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"The proportion of high risk patients is higher in the predicted higher risk categories. This means lower false negatives.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"If risk stratification is the key desired output, then further refining the regression model may be the better approach to improving the overall performance of the system.\"\n      }), \"\\n\", _jsxs(_components.h2, {\n        children: [\"7. Conclusions\", _jsx(\"a\", {\n          name: \"conclusions\"\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"A number of baseline machine learning models were trained on EPR data from GHNHSFT.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The best performing regression model achieved a Mean Absolute Error of 4.1 days, compared to 3.8 days for previous work using a convolutional neural network.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Simpler baseline models benefit from enhanced explainability and less compute resources for training. In this case the most important features were related to age and serious illness.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The overall performance of the best regression model was still poor - despite an MAE of 4.1 days, the model failed to capture long stayers and requires further work before use.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The best performing classification model achieved a weighted F1 score of 0.6.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The overall performance of the best classification model was poor - the model failed to capture high risk and assigned a high proportion (\u003e50%) of very low risk patients to higher risk groups.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Using the regression model to calculate equivalent risk scores led to a better risk stratification model, where only ~20% of very low risk patients were assigned to the high risk group.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Demographic analysis showed that the model performed differently for different ethnicities and indices of multiple deprivation, but both model performance needs to be improved and sample sizes need to be increased in order to draw any meaning from these initial findings.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"There is opportunity for much future work, which should be balanced with the utility of these predictions in the clinical context.\"\n      }), \"\\n\", _jsxs(_components.h2, {\n        children: [\"8. Future work\", _jsx(\"a\", {\n          name: \"futurework\"\n        })]\n      }), \"\\n\", _jsx(_components.h3, {\n        children: \"Modelling improvements\"\n      }), \"\\n\", _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"Feature engineering of free text fields. Early on we decided to focus on simple numerical and categorical features for this project. A huge amount of rich data is present in fields such as \", _jsx(_components.code, {\n            children: \"presenting_complaint\"\n          }), \" and \", _jsx(_components.code, {\n            children: \"reason_for_admission\"\n          }), \".\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Including features available after admission. Fields such as \", _jsx(_components.code, {\n            children: \"all_diagnoses\"\n          }), \" and \", _jsx(_components.code, {\n            children: \"all_treatments\"\n          }), \" will provide clinically important information, and may improve the performance of the predictions.\"]\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Focussing on a smaller number of features. Once the most important features are identified, a model using the top e.g. 10 features could be trained and tested.\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Building two models - one for short stay and one for long stay. This may help capture the bimodal nature of the underlying dataset.\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Including \", _jsx(_components.code, {\n            children: \"MINOR\"\n          }), \" cases. This project focussed on \", _jsx(_components.code, {\n            children: \"MAJOR\"\n          }), \", \", _jsx(_components.code, {\n            children: \"non-elective\"\n          }), \" cases. 70%+ of the original data belonged to minor cases, and in combination with the above, including this data could lead to an improvement in model performance.\"]\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Treating Length of Stay as a discrete variable and applying poisson distribution appropriate approaches to modelling.\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Exploring Generalised Linear Models using e.g. \", _jsx(_components.a, {\n            href: \"https://pygam.readthedocs.io/en/latest/index.html\",\n            children: \"pyGAM\"\n          }), \".\"]\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Exploring Bayesian approaches.\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Exploring the addition of latent variable(s).\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.h3, {\n        children: \"Demographic analysis improvements\"\n      }), \"\\n\", _jsxs(_components.ol, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"Statistical testing of fairness. Once model performance reaches a sufficient level, further statistical tests of model performance across demographics should be conducted using e.g. a two-sided Kolmogorov-Smirnov test.\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Combine smaller groups. For example, grouping \", _jsx(_components.code, {\n            children: \"British\"\n          }), \" and \", _jsx(_components.code, {\n            children: \"Non-British\"\n          }), \" ethnicities would allow statistical comparisons to be made between the majority group and other groups.\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.h3, {\n        children: \"Technical improvements\"\n      }), \"\\n\", _jsxs(_components.ol, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"Move from Notebooks to python scripts. Jupyter Notebooks are an excellent exploratory tool, but do not work well with version control or automated testing.\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Implement a \", _jsx(_components.a, {\n            href: \"https://github.com/NHSDigital/rap-community-of-practice\",\n            children: \"Reproducible Analytical Pipeline\"\n          }), \". This will allow reuse of the approaches here and improve overall code quality.\"]\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Abstract visualisation code into functions. This will improve readability of the code.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.h4, {\n        children: \"Acknowledgments\"\n      }), \"\\n\", _jsxs(_components.ol, {\n        children: [\"\\n\", _jsx(_components.li, {\n          children: \"Joe Green, GHNHSFT for presenting the challenge to Skunkworks and supporting problem definition/data selection\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Tom Lane, GHNHSFT for support in final stages\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Brad Pearce and Peter Coetzee, Polygeist, for the original CNN-based model\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Jennifer Hall, Matthew Cooper and Sanson Poon, NHS AI Lab Skunkworks for guidance, code and report review\"\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Chris Mainey, NHSE, for suggestions of additional modelling improvements\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.table, {\n        children: [_jsx(_components.thead, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.th, {\n              children: \"Output\"\n            }), _jsx(_components.th, {\n              children: \"Link\"\n            })]\n          })\n        }), _jsx(_components.tbody, {\n          children: _jsxs(_components.tr, {\n            children: [_jsx(_components.td, {\n              children: \"Open Source Code \u0026 Documentation\"\n            }), _jsx(_components.td, {\n              children: _jsx(_components.a, {\n                href: \"https://github.com/nhsx/skunkworks-long-stayer-risk-stratification-baseline\",\n                children: \"Github\"\n              })\n            })]\n          })\n        })]\n      })]\n    });\n  }\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{"title":"Long Stayer Risk Stratification Baseline Models","summary":"Baseline machine learning models using historical data from Gloucestershire Hospitals NHS Foundation Trust to predict how long a patient will stay in hospital upon admission.","category":"Projects"}},"meta":{"title":"Long Stayer Risk Stratification Baseline Models","summary":"Baseline machine learning models using historical data from Gloucestershire Hospitals NHS Foundation Trust to predict how long a patient will stay in hospital upon admission.","category":"Projects"}},"__N_SSG":true},"page":"/[slug]","query":{"slug":"long-stay-baseline"},"buildId":"g3Eb-oK66BnMwIyIlPHDf","assetPrefix":"/skunkworks","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>